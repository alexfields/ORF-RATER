#! /usr/bin/env python

import argparse
import sys
from collections import defaultdict
from yeti.genomics.roitools import Transcript, SegmentChain
import pandas as pd
import numpy as np
import multiprocessing as mp
import subprocess as sp
import os
from time import strftime

parser = argparse.ArgumentParser(description='Identify annotated CDSs among the ORFs identified by find_orfs.py, possibly from multiple source '
                                             'files. These CDSs will be used to construct metagenes for regression and as the gold positive set for '
                                             'the random forest classifier.')
parser.add_argument('--tfamstem', default='tfams', help='Transcript family information generated by make_tfams.py. Both TFAMSTEM.txt and '
                                                        'TFAMSTEM.bed should exist. (Default: tfams)')
parser.add_argument('--orfstore', default='orf.h5',
                    help='Path to pandas HDF store containing ORFs to regress; generated by find_orfs.py (Default: orf.h5)')
parser.add_argument('--cdsstore', default='cds.h5',
                    help='File to which to output table with CDS information, formatted as pandas HDF store (table name is "annot_CDSs"). Columns '
                         'indicate general information as well as the transcript ID in the annotation BED file from which the CDS originated. '
                         '(Default: cds.h5)')
parser.add_argument('--inbed', default='transcripts.bed', help='Transcriptome BED-file. Annotated CDSs are assumed to be bona fide CDSs, unless '
                                                               '--ignoreannotations is set. (Default: transcripts.bed)')
parser.add_argument('--ignoreannotations', action='store_true', help='If flag is set, CDS annotations in INBED will be ignored. Typically used in '
                                                                     'conjunction with --extracdsbeds')
parser.add_argument('--extracdsbeds', nargs='+', help='Extra bed file(s) containing additional annotated CDSs beyond (or instead of) those in inbed. '
                                                      'If transcript names are repeated across these files, sources of annotated CDSs may become '
                                                      'ambiguous, but no error or warning will be triggered.')
parser.add_argument('-v', '--verbose', action='count',
                    help='Output a log of progress and timing (to stdout). Repeat for higher verbosity level.')
parser.add_argument('-p', '--numproc', type=int, default=1, help='Number of processes to run. Defaults to 1 but more recommended if available.')
parser.add_argument('-f', '--force', action='store_true', help='Force file overwrite')
opts = parser.parse_args()

if not opts.force and os.path.exists(opts.cdsstore):
    raise IOError('%s exists; use --force to overwrite' % opts.orfstore)

if opts.verbose:
    sys.stdout.write(' '.join(sys.argv) + '\n')

    def logprint(nextstr):
        sys.stdout.write('[%s] %s\n' % (strftime('%Y-%m-%d %H:%M:%S'), nextstr))
        sys.stdout.flush()

    log_lock = mp.Lock()

# hash transcripts by ID for easy reference later
with open(opts.inbed, 'rU') as inbed:
    bedlinedict = {line.split()[3]: line for line in inbed}

tfamtids = defaultdict(list)
with open('%s.txt' % opts.tfamstem, 'rU') as tfamtable:
    for line in tfamtable:
        ls = line.strip().split()
        tfamtids[ls[1]].append(ls[0])

with open('%s.bed' % opts.tfamstem, 'rU') as tfambed:
    tfambedlines = {line.split()[3]: line for line in tfambed}

if not opts.ignoreannotations:
    annot_tfam_lookups = [tfamtids]
    annot_tid_lookups = [bedlinedict]
else:
    annot_tfam_lookups = []
    annot_tid_lookups = []

if opts.extracdsbeds:
    if opts.verbose:
        logprint('Identifying tfams for extra CDS annotations')
    import pybedtools  # to handle identifying which tfams get the extra CDSs - otherwise would need to replicate a lot of intersection functionality
    tfambedtool = pybedtools.BedTool('%s.bed' % opts.tfamstem)
    for cdsbedfile in opts.extracdsbeds:
        with open(cdsbedfile, 'rU') as cdsbed:
            annot_tid_lookups.append({line.split()[3]: line for line in cdsbed})  # as usual, hash bed lines by transcript ID
        annot_tfam_lookups.append(defaultdict(list))
        for line in tfambedtool.intersect(pybedtools.BedTool(cdsbedfile), split=True, s=True, wa=True, wb=True):
            annot_tfam_lookups[-1][line[3]].append(line[15])
# after this has finished, each element of annot_tfam_lookup will be a dictionary mapping tfams to lists of transcript IDs in the annotation bed files
# similarly, each element of annot_tid_lookup will map transcript IDs to BED lines

tfams_with_annots = set(sum([x.keys() for x in annot_tfam_lookups], []))


def _find_annots(tfam, tfam_orfs):
    """Identify the annotated CDSs within a given transcript family, divided into those that match ORFs identified in find_orfs.py and those that do
    not (due to being on a transcript isoform not present in the transcriptome in use)."""
    currtfam = SegmentChain.from_bed(tfambedlines[tfam])
    chrom = currtfam.chrom
    strand = currtfam.strand
    tfam_genpos = np.array(currtfam.get_position_list(stranded=True))
    # annot_CDS_dfs = []
    found_cds_info = []
    unfound_cds_info = []
    for (annot_tfam_lookup, annot_tid_lookup) in zip(annot_tfam_lookups, annot_tid_lookups):
        if tfam in annot_tfam_lookup:
            for (annot_tidx, annot_tid) in enumerate(annot_tfam_lookup[tfam]):
                curr_trans = Transcript.from_bed(annot_tid_lookup[annot_tid])
                if curr_trans.cds_start is not None and curr_trans.cds_end is not None:
                    found = False
                    curr_gcoord = curr_trans.get_genomic_coordinate(curr_trans.cds_start)[1]
                    curr_gstop = curr_trans.get_genomic_coordinate(curr_trans.cds_end-1)[1]+(strand == '+')*2-1
                    shared_start = (tfam_orfs['gcoord'] == curr_gcoord)
                    shared_stop = (tfam_orfs['gstop'] == curr_gstop)
                    curr_cds_pos = curr_trans.get_cds().get_position_set()
                    shared_len = (tfam_orfs['tstop']-tfam_orfs['tcoord'] == len(curr_cds_pos))
                    possible_orfs = shared_start & shared_stop & shared_len
                    if possible_orfs.any() and curr_cds_pos.issubset(tfam_genpos):
                        for (orfname, tid, tcoord, tstop) in tfam_orfs.loc[possible_orfs, ['orfname', 'tid', 'tcoord', 'tstop']].itertuples(False):
                            if curr_cds_pos.issubset(SegmentChain.from_bed(bedlinedict[tid]).get_genomic_coordinate(np.arange(tcoord, tstop))[1]):
                                # this actually tests for equality, as they are guaranteed the same length
                                found = True
                                found_cds_info.append((tfam,
                                                       annot_tid,
                                                       chrom,
                                                       curr_gcoord,
                                                       curr_gstop,
                                                       strand,
                                                       len(curr_cds_pos)/3-1,
                                                       orfname))
                                break
                    if not found:
                        unfound_cds_info.append((tfam,
                                                 annot_tid,
                                                 chrom,
                                                 curr_gcoord,
                                                 curr_gstop,
                                                 strand,
                                                 len(curr_cds_pos)/3-1))
    return (pd.DataFrame(found_cds_info, columns=['tfam', 'tid', 'chrom', 'gcoord', 'gstop', 'strand', 'AAlen', 'orfname']),
            pd.DataFrame(unfound_cds_info, columns=['tfam', 'tid', 'chrom', 'gcoord', 'gstop', 'strand', 'AAlen']))


def _find_annots_by_chrom(chrom_to_do):
    """Identify annotated CDSs on all of the transcripts on a given chromosome"""
    named_orfs = pd.read_hdf(opts.orfstore, 'all_orfs', where="chrom == '%s' and tstop > 0" % chrom_to_do,
                             mode='r', columns=['tfam', 'tmap', 'tid', 'tcoord', 'tstop', 'chrom', 'gcoord', 'gstop', 'strand', 'orfname']) \
        .drop_duplicates('orfname')
    named_orfs = named_orfs[named_orfs['tfam'].isin(tfams_with_annots)]  # don't bother looking if there aren't any annotated CDSs to be found
    if not named_orfs.empty:
        res = tuple([pd.concat(dfs, ignore_index=True) for dfs in zip(*[_find_annots(tfam, tfam_orfs)
                                                                        for (tfam, tfam_orfs) in named_orfs.groupby('tfam')])])
        if opts.verbose > 1:
            with log_lock:
                logprint('%s complete' % chrom_to_do)
        return res
    else:
        if opts.verbose > 1:
            with log_lock:
                logprint('Found no annotated CDSs on %s' % chrom_to_do)
        return pd.DataFrame(), pd.DataFrame()
    
if opts.verbose:
    logprint('Identifying annotated CDSs by chromosome')

with pd.get_store(opts.orfstore, mode='r') as orfstore:
    chroms = orfstore.select('all_orfs/meta/chrom/meta').values  # because saved as categorical, this is a list of all chromosomes
workers = mp.Pool(opts.numproc)
(found_cds, unfound_cds) = [pd.concat(dfs, ignore_index=True) for dfs in zip(*workers.map(_find_annots_by_chrom, chroms))]
workers.close()

if opts.verbose:
    logprint('Saving results')

for catfield in ['chrom', 'strand']:
    found_cds[catfield] = found_cds[catfield].astype('category')  # saves disk space and read/write time
    unfound_cds[catfield] = unfound_cds[catfield].astype('category')  # saves disk space and read/write time

origname = opts.cdsstore+'.tmp'
with pd.get_store(origname, mode='w') as outstore:
    outstore.put('found_cds', found_cds, format='t', data_columns=True)
    outstore.put('unfound_cds', unfound_cds, format='t', data_columns=True)
sp.call(['ptrepack', origname, opts.cdsstore])  # repack for efficiency
os.remove(origname)

if opts.verbose:
    logprint('Tasks complete')
